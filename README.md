
About Dataset Brief

This research paper presents the development of a robust medical question-answer information base, created to facilitate a data-driven approach in the field of healthcare. Leveraging a multi-source strategy, we meticulously collected information from Chinese medical literature, expert consultations with over 6000 specialists, and data shared by several reputable hospitals. All information was translated into English, ensuring accessibility and usability for a wider audience. Our information base construction was guided by a three-fold process:

 Collection of data from diverse Chinese medical books, allowing us to encompass a broad spectrum of medical knowledge and practices.
 Expert consultations were conducted with an extensive panel of medical specialists, ranging from various domains of medicine, to ensure the accuracy and reliability of the gathered information.
 Collaborations with reputable hospitals enriched the database with real-world case studies and empirical insights.


The culmination of our efforts resulted in a comprehensive and versatile medical question-answer information base. The repository now contains a vast array of medical topics, encompassing diagnostics, treatments, pharmaceutical insights, and patient care. The information base is designed to serve as a valuable resource for data-driven research and the testing of medical question-answering models. While our information base presents a significant advancement in medical knowledge accessibility, we emphasize the importance of seeking professional medical advice. Our repository should be considered as an auxiliary tool for research and educational purposes, and we strongly recommend consulting qualified medical professionals before making any medical decisions.
and data set available to the research community upon request.



The dataset used in this research is publicly available on GitHub and can be accessed at https://github.com/zxzxzx77/ENTITY-NAMING-. This repository contains the raw data and essential resources needed to replicate the experiments conducted in this paper.

Pre-processing:
To ensure the dataset is usable for the experiments, the following pre-processing steps were applied:

Data Cleaning: The raw dataset contained a collection of medical questions, answers, and relevant case studies. We removed any incomplete or erroneous entries and filtered out duplicate records to maintain data integrity.
Tokenization: Each sentence in the dataset was tokenized using standard natural language processing tokenizers. For this task, we employed tokenizers from pre-trained models, such as Word2Vec and GloVe, to convert the raw text into tokenized sequences of words or subwords. These tokenized sequences served as input to the model.
Handling Imbalanced Data: The dataset was evaluated for class imbalances. In cases where specific medical entities were underrepresented, we applied oversampling techniques to ensure the model would not be biased toward certain classes of entities.
Polysemy and Ambiguity Resolution: Since polysemy presented a challenge in the dataset, we employed context-sensitive embeddings, such as those generated by BERT and GPT-2, to better capture the meaning of words in context. This step was crucial for ensuring the model correctly disambiguated words with multiple meanings.
Splitting the Data: The dataset was split into training, validation, and test sets using an 80-10-10 ratio. This split ensured that the model was trained on a substantial portion of the data while leaving enough samples for validation and testing.
Word Embeddings and Encoding: Each word or token was converted into a vector representation using pre-trained word embeddings (e.g., Word2Vec, GloVe). For models like BERT and GPT-2, the tokenized input was passed through the model to obtain context-aware embeddings.
Instructions for Reproduction
Researchers interested in reproducing the experiments can follow these steps:

Clone the Repository: Download the dataset and pre-processing scripts from the GitHub repository:


git clone https://github.com/zxzxzx77/ENTITY-NAMING-.git
Install Dependencies: Ensure that you have the required libraries and frameworks, such as TensorFlow, PyTorch, Hugging Face's transformers library, and sklearn for data splitting. A requirements.txt file is provided in the repository.
Run Pre-processing Scripts: The repository includes Python scripts for data pre-processing. These scripts clean, tokenize, and convert the raw text into the format required by the model. Example command:


python preprocess_data.py --input dataset/raw_data.csv --output dataset/processed_data.csv
Load the Pre-trained Models: The experiments used pre-trained models like BERT, GPT-2, and ELECTRA alongside the proposed T-MFRNN architecture. The repository includes code to load and fine-tune these models on the pre-processed dataset.
Train and Evaluate: The scripts provided will enable you to train the model on the dataset and evaluate its performance using metrics such as accuracy, precision, recall, and F1-score. The training and evaluation can be run with the following command:


python train_model.py --model bert --epochs 3 --batch_size 32
By following these steps, researchers can replicate the results presented in this paper and adapt the model for their own experiments.
